\section{Clustering}
\textit{Given a training set, group the data into a few clusters. }

\textbf{Data Representation:}
\begin{itemize}
	\item Vector data: $n$ vecotrs in $\R^d$
	\item Histogram data:  $n$ histograms in $\R^d$
	\item Proximity data:  $n \times n$ pairwise proximity matrix. Much harder problem (structure hidden in $n^2$ pairwise relations

\end{itemize}
\subsection{$k$-means vs. EM}
\begin{tabular}{ p{0.45\columnwidth} | p{0.45\columnwidth} }
		\textbf{k-means} & \textbf{EM}\\\hline
		 \textbf{Objective: } minimize inter-cluster variance & \\\hline
		 Hard assignment & Soft assignment \\\hline	
  		works well for homog. clusters (assumes spherical clusters, with equal covariance matrices) & Can constrain algorithm to get different shapes of cov-matrices (not limited to spherical shapes)\\\hline
\end{tabular}

Neither of the algorithms can detect outliers! This would need a preprocessing step. In the presence of outliers, EM is more sensitive, since there are no constraints on the covariance matrix.

\subsection{$k$-means}
\subsubsection{$k$-means Problem}
\textit{Group data into $k$ groups.}

\begin{itemize}
	\item Given: $d$-dimensional sample vectors $\X$
	\item Assignment function
	\begin{equation*}
		\begin{gathered}
			c: \R^d \to \{1, ..., k\} \\
			\x \mapsto c(\x)
		\end{gathered}
	\end{equation*}
	\item Prototypes $\mu_c \in \mathcal Y \subset \R^d$
	\item \textbf{Problem: } find $c(.)$ and $\mathcal Y$ that minimize
	$$
		\mathcal R^{km}(c,\mathcal Y) = \sum_{x\in X}\norm{x - \mu_{c(x)}}^2
	$$
\end{itemize}

\subsubsection{$k$-means algorithm}

\begin{algorithm}[H]  
	\SetKwInOut{Input}{input}
	\SetKwInOut{Init}{init}

	\Input{$\mathcal X =\{\x_1, ..., \x_n\}$}
	\Init{$\mu_c =\x_c$ for $ 1\leq c\leq k$}

	\RepeatUntil{Changes of $c(\x), \mathcal Y$ vanish}{
		Keep prototypes $\mathcal Y$ fixed and assign sample vector $\x$ to neareast prototype 
		$$
			c(\x) \in \argmin_{c\in \{1, ..., k\}} \norm{\x-\mu_c}^2
		$$ \\
		Keep assignments $c(\x)$ fixed and estimate prototypes 
		$$
			\mu_\alpha = \frac{1}{n_\alpha} \sum_{\x:c(\x)=\alpha} \x, \text{ with $n_\alpha = \text{\#}\{\x:c(\x) = \alpha\}$}
		$$
  	}
  	\Return{$c(\x), \forall \x\in \mathcal X$ and the prototypes $\mathcal Y$}
  \caption{$k$-means algorithm}
\end{algorithm}

\subsection{Mixture Model}
Data are assumed to be distributed according to a density. When multiple sources are considered as potential causes for an observed example, this is a mixture model with the density for a feature vector $\x$:
$$
	p(\x|\pi_1, ..., \pi_k, \theta_1, ..., \theta_k) = \sum_{c\leq k} \pi_c p(\x|\theta_c)
$$
the mixture weight $\pi_c$ is the prior probability that a sample is generated by the mixture component $c$ with parameters $\theta_c$ (i.e. $\pi_c = p(c(\x) = c, \theta_c$).

\subsubsection{Gaussian Mixtures}
\textit{$\x$ was drawn from one of $k$ gaussians, depending on the class .}
\begin{itemize}
	\item Parameters $\theta = (\mu, \Sigma)$
	$$
		p(\x|\mu,\Sigma) = \frac{1}{\sqrt{2\pi}^d}\frac{1}{\sqrt{\|\Sigma|}}\exp\left(-\frac{1}{2}(\x - \mu)^T\Sigma^{-1}(\x - \mu) \right)
	$$
	\item Estimate $\hat\theta$ that maximizes the likelihood of sample feature vectors $\mathcal X = \{\x_1, ..., \x_n\}$
	$$
			p(\mathcal X|\pi_1, ..., \pi_k, \theta_1, ..., \theta_k) = \prod_{x\in \mathcal X}\sum_{c\leq k} \pi_c p(\x|\theta_c)
	$$
	\item Log-Likelihood 
	$$
	L(\mathcal X|\pi_1, ..., \pi_k, \theta_1, ..., \theta_k) = \sum_{x\in \mathcal X}\log \sum_{c\leq k}\pi_cp(\x|\theta_c)
	$$
\end{itemize}
Direct optimization of the log-likelihood is intractable due to sum within the logarithm (i.e. there is \textbf{no closed form solution}). 

EM Mixture models solve this by introducing latent indicator variables for mode assignments and maximizing the joint likelihood of the observable and latent variables.

\subsection{Expectation - Maximization algorithm}

\textbf{Principle: }
\begin{enumerate}
	\item Calculate $Q(\theta; \theta^{(j)}) = \E_{\mathcal X_L}\left[L(\mathcal X, \mathcal X_L|\theta) \middle\vert \mathcal X, \theta^{(j)}\right]$ \\
		\textit{Function of parameters given the parameters in the step before.}
	\item Estimate new parameters by maximizing the log-likelihood of $Q$: 
	$$\theta^{(j+1)}\in \argmax_{\theta}Q(\theta; \theta^{(j)})$$
	\item Repeat until convergence
\end{enumerate}
\textit{\textbf{Derivations: } (Full derivations on p. 11 - 16, Lecture 12 - clustering)}
$$
	M_{\x c} = 
	\begin{cases}
		1 &\textit{Mode $c$ has generated vector $\x$} \\
		0 &\textit{Mode $c$ has not generated vector $\x$}
	\end{cases}
$$

This gives
\begin{align*}
	P(\mathcal X, M|\theta) &= \prod_{x\in\mathcal X}\prod_{c=1}^k(\pi_c P(\x|\theta_c))^{M_{\x c}}\\
	L(\mathcal X, M|\theta) &= \log P(\mathcal X, M|\theta) =\sum_{x\in\mathcal X}\sum_{c=1}^k M_{\x c}(\pi_c P(\x|\theta_c))
\end{align*}

We define
$
	\gamma_{\x c} := \E_M\left[M_{\x c}\middle \vert \mathcal X, \theta^{(j)}\right]
$



\begin{algorithm}[H]  
	
	\RepeatUntil{Changes are small enough}{
		\textbf{E-Step: For all $c$}
		$$
			\gamma_{\x c} = \frac{P(\x|c, \theta^{(j)}P(c|\theta^{(j)})}{P(\x|\theta^{(j)})}
		$$	\\
		\textbf{M-Step: For all $c$}
		\begin{align*}
			\mu_c^{(j+1)} &= \frac{\sum_{\x\in \mathcal X}\gamma_{\x c}\x}{\sum_{\x\in \mathcal X}\gamma_{\x c}} \\
			(\sigma_c^2)^{(j+1)} &= \frac{\sum_{\x\in \mathcal X}\gamma_{\x c}(\x-\mu_c)^2}{\sum_{\x\in \mathcal X}\gamma_{\x c}} \\
			\pi_c^{(j+1)} &= \frac{1}{|\mathcal X|}\sum_{\x\in \mathcal X}\gamma_{\x c}
		\end{align*}	
  	}
  \caption{Expectation-Maximization Algorithm}
\end{algorithm}

\subsection{Problems with Mixtures of Gaussians}
\begin{itemize}
	\item Computation time: Time to estimate the model parameters may be large
	\item Number of free parameters: Scales with $O(d^2)$ ($d$ data dimension)
	\begin{itemize}
		\item[$\Rightarrow$] Param. estimation problematic if dimension of features space is high and number of samples is slow
		\item[$\Rightarrow$] \textbf{Solution: } Use only hard assignments $c(x) \in \{1, ..., k\}$ as in $k$-means clustering.
	\end{itemize}	
\end{itemize}

